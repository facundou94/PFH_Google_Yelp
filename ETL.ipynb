{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import gdown\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews Estados - Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar archivos (cambiar a la nube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo y almacenar cada línea en una lista\n",
    "lines = []\n",
    "\n",
    "for i in range (18):\n",
    "    file='reviews-estados/review-California/'+str(i+1)+'.json'\n",
    "    with open(file, 'r') as file:\n",
    "        for line in file:\n",
    "            lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir todas las líneas en un solo string y leerlo como un DataFrame\n",
    "all_lines = ''.join(lines)\n",
    "df_reviews = pd.read_json(StringIO(all_lines), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lines\n",
    "del all_lines\n",
    "del file\n",
    "del line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Espacio que ocupan\n",
    "Carpeta: 763 Mb (3,8 segundos en importar para una lista, 48,44 segundos para pasarlo a dataframe)\n",
    "Dataframe: 164,8 Mb\n",
    "CSV: 545 Mbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata  - Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar los archivos desde la nube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tamaño:\n",
    "carpeta: 2,76 Gb\n",
    "dataframe 370 Mb\n",
    "csv 2,3 Gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1OnyhmyG8xzdn4XU9LYcnwzBseB1_rChS\n",
      "From (redirected): https://drive.google.com/uc?id=1OnyhmyG8xzdn4XU9LYcnwzBseB1_rChS&confirm=t&uuid=03f8792e-1011-4b2d-8196-607d15726cee\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\1.json\n",
      "100%|██████████| 256M/256M [00:06<00:00, 37.7MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=15D_5EkxqPP0XJb5I5bYI8b1wQV7B2fx_\n",
      "From (redirected): https://drive.google.com/uc?id=15D_5EkxqPP0XJb5I5bYI8b1wQV7B2fx_&confirm=t&uuid=ab4b6a9d-8aee-4153-9cc4-73dc4064ccae\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\2.json\n",
      "100%|██████████| 257M/257M [00:09<00:00, 27.3MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1fDBVCmf4JA7gkIyjpv5mHEMySb19C-vz\n",
      "From (redirected): https://drive.google.com/uc?id=1fDBVCmf4JA7gkIyjpv5mHEMySb19C-vz&confirm=t&uuid=d854ef6a-e45c-47f1-b98a-9a99e26300d0\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\3.json\n",
      "100%|██████████| 259M/259M [00:07<00:00, 33.1MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Mj2oUZy5gGznhthcUGi8_sgKhBwypE74\n",
      "From (redirected): https://drive.google.com/uc?id=1Mj2oUZy5gGznhthcUGi8_sgKhBwypE74&confirm=t&uuid=1067e5be-653b-4575-b702-a927be1436ff\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\4.json\n",
      "100%|██████████| 262M/262M [00:07<00:00, 34.1MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1IXok40Zp61CGwFDgyvLUwV02c4BWGrjj\n",
      "From (redirected): https://drive.google.com/uc?id=1IXok40Zp61CGwFDgyvLUwV02c4BWGrjj&confirm=t&uuid=616a04e7-8512-4e64-a1df-6f680d3a8e7b\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\5.json\n",
      "100%|██████████| 264M/264M [00:06<00:00, 38.4MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1UmsN_ZOFQqVl7W9SbnxHkSQavo1_Iwqx\n",
      "From (redirected): https://drive.google.com/uc?id=1UmsN_ZOFQqVl7W9SbnxHkSQavo1_Iwqx&confirm=t&uuid=1cfd5d8f-386d-4962-8bbc-d5080f9b31d5\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\6.json\n",
      "100%|██████████| 267M/267M [00:09<00:00, 28.9MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1KfQBhJlnuziKjf-9haQGaiPtCPnUUDla\n",
      "From (redirected): https://drive.google.com/uc?id=1KfQBhJlnuziKjf-9haQGaiPtCPnUUDla&confirm=t&uuid=8f8f6dc3-bf3f-47fa-bad8-8ae9d92ac694\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\7.json\n",
      "100%|██████████| 273M/273M [00:11<00:00, 22.9MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1ebTUx2klGy7L9lGlZl3GCPXxSwSD55vX\n",
      "From (redirected): https://drive.google.com/uc?id=1ebTUx2klGy7L9lGlZl3GCPXxSwSD55vX&confirm=t&uuid=e1d23c9f-48ed-41e6-8a41-d812044b1e0a\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\8.json\n",
      "100%|██████████| 284M/284M [00:07<00:00, 38.1MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1td6twU60LAS-z5mB0MeSJEpGhH7jcGKm\n",
      "From (redirected): https://drive.google.com/uc?id=1td6twU60LAS-z5mB0MeSJEpGhH7jcGKm&confirm=t&uuid=e56dec2b-58a8-49f2-8af3-c74e1b225d46\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\9.json\n",
      "100%|██████████| 277M/277M [00:11<00:00, 23.1MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1NQgHgNm9PV8MSiOXNoQ2UkIF9e5AdLk7\n",
      "From (redirected): https://drive.google.com/uc?id=1NQgHgNm9PV8MSiOXNoQ2UkIF9e5AdLk7&confirm=t&uuid=cda88958-c5b3-4495-963e-04d20c30c5e6\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\10.json\n",
      "100%|██████████| 282M/282M [00:08<00:00, 31.9MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1GYwWfH7EvWMZn14vQRNr5CjEely4eWrB\n",
      "From (redirected): https://drive.google.com/uc?id=1GYwWfH7EvWMZn14vQRNr5CjEely4eWrB&confirm=t&uuid=a03baabd-a7c2-4d54-807d-febc9e8e1522\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\11.json\n",
      "100%|██████████| 288M/288M [00:09<00:00, 30.7MB/s] \n"
     ]
    }
   ],
   "source": [
    "## descargados directamente desde google drive\n",
    "\n",
    "# IDs de los archivos de Google Drive\n",
    "file_links = [\n",
    "    'https://drive.google.com/file/d/1OnyhmyG8xzdn4XU9LYcnwzBseB1_rChS', \n",
    "    'https://drive.google.com/file/d/15D_5EkxqPP0XJb5I5bYI8b1wQV7B2fx_', \n",
    "    'https://drive.google.com/file/d/1fDBVCmf4JA7gkIyjpv5mHEMySb19C-vz', \n",
    "    'https://drive.google.com/file/d/1Mj2oUZy5gGznhthcUGi8_sgKhBwypE74', \n",
    "    'https://drive.google.com/file/d/1IXok40Zp61CGwFDgyvLUwV02c4BWGrjj',\n",
    "    'https://drive.google.com/file/d/1UmsN_ZOFQqVl7W9SbnxHkSQavo1_Iwqx', \n",
    "    'https://drive.google.com/file/d/1KfQBhJlnuziKjf-9haQGaiPtCPnUUDla', \n",
    "    'https://drive.google.com/file/d/1ebTUx2klGy7L9lGlZl3GCPXxSwSD55vX', \n",
    "    'https://drive.google.com/file/d/1td6twU60LAS-z5mB0MeSJEpGhH7jcGKm', \n",
    "    'https://drive.google.com/file/d/1NQgHgNm9PV8MSiOXNoQ2UkIF9e5AdLk7', \n",
    "    'https://drive.google.com/file/d/1GYwWfH7EvWMZn14vQRNr5CjEely4eWrB'\n",
    "]\n",
    "\n",
    "# Nombres de los archivos locales (presumiendo que siguen el patrón 1.json, 2.json, ..., 11.json)\n",
    "file_names = [f'{i}.json' for i in range(1, len(file_links) + 1)]\n",
    "\n",
    "# Inicializar una lista para almacenar los DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Descargar y leer cada archivo JSON\n",
    "for file_link, file_name in zip(file_links, file_names):\n",
    "    try:\n",
    "        # Obtener el ID del archivo desde el enlace\n",
    "        file_id = file_link.split('/d/')[1].split('/')[0]\n",
    "        download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "        \n",
    "        # Descargar el archivo\n",
    "        gdown.download(download_url, file_name, quiet=False)\n",
    "        \n",
    "        # Verificar si el archivo descargado es un JSON válido\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            first_char = file.read(1)\n",
    "            if not first_char:\n",
    "                raise ValueError(f\"El archivo {file_name} está vacío.\")\n",
    "            file.seek(0)\n",
    "        \n",
    "        # Leer el archivo JSON en un DataFrame\n",
    "        df = pd.read_json(file_name, lines=True)\n",
    "        \n",
    "        # Agregar el DataFrame a la lista\n",
    "        dataframes.append(df)\n",
    "        \n",
    "        # Eliminar el archivo descargado\n",
    "        os.remove(file_name)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el archivo {file_name}: {e}\")\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_metadataGoog = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrar por estado california"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraer el estado de la dirección\n",
    "df_metadataGoog['estado'] = df_metadataGoog['address'].str.extract(r', ([A-Z]{2}) \\d{5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtro los datos de California, para liberar espacio\n",
    "df_metadatosCA = df_metadataGoog[df_metadataGoog['estado'] == 'CA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimino el dataframe que tiene los metadatos de todos los estados, para liberar memoria\n",
    "del df_metadataGoog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadatosCA.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feder\\AppData\\Local\\Temp\\ipykernel_7120\\1024337664.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_metadatosCA.drop('index', axis='columns', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_metadatosCA.drop('index', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportar el archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardo el archivo parquet, para poder importarlo si es necesario\n",
    "df_metadatosCA.to_parquet('Archivos/metadatosCA.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business - YELP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar los archivos desde la nube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1byFtzpZXopdCN-XYmMHMpZqzgAqfQBBu\n",
      "From (redirected): https://drive.google.com/uc?id=1byFtzpZXopdCN-XYmMHMpZqzgAqfQBBu&confirm=t&uuid=69cf883e-c12b-4736-b320-965493d099b8\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\business.pkl\n",
      "100%|██████████| 116M/116M [00:03<00:00, 36.9MB/s] \n"
     ]
    }
   ],
   "source": [
    "# ID del archivo de Google Drive\n",
    "file_id = '1byFtzpZXopdCN-XYmMHMpZqzgAqfQBBu'\n",
    "# Construir la URL de descarga\n",
    "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "# Descargar el archivo usando gdown\n",
    "file_path = 'business.pkl'\n",
    "gdown.download(download_url, file_path, quiet=False)\n",
    "\n",
    "# Cargar el archivo pickle en un DataFrame de pandas\n",
    "df_businessYelp = pd.read_pickle(file_path)\n",
    "\n",
    "# Eliminar el archivo descargado\n",
    "os.remove(file_path)"
   ]
  },

{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Business - YELP\n",
    "### Importación de los Datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del archivo .pkl\n",
    "ruta_pkl = 'D:/usuarios/CIN/MIS DOCUMENTOS/Proyecto_Final/set_de_datos_Yelp/business.pkl'\n",
    "\n",
    "# Cargar el archivo .pkl en un DataFrame de pandas\n",
    "df_business = pd.read_pickle(ruta_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de los Datos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina las columnas duplicadas\n",
    "df = df_business.loc[:, ~df_business.columns.duplicated()]\n",
    "\n",
    "#  Filtra negocios en un estado específico (por ejemplo, 'CA' para California)\n",
    "business_california = df[df['state'] == 'CA']\n",
    "\n",
    "# Elimina valores nulos en las columnas attributes y hours\n",
    "business_california_sn= business_california.dropna(subset=['attributes', 'hours'])\n",
    "\n",
    "# Modificación del tipo  de datos en las columnas 'latitude','longitud','stars','review_count','is-open'\n",
    "\n",
    "business_california_sn['latitude'] = pd.to_numeric(business_california_sn['latitude'], errors='coerce')\n",
    "business_california_sn['longitude'] = pd.to_numeric(business_california_sn['longitude'], errors='coerce')\n",
    "business_california_sn['stars'] = pd.to_numeric(business_california_sn['stars'], errors='coerce')\n",
    "business_california_sn['review_count'] = pd.to_numeric(business_california_sn['review_count'], errors='coerce')\n",
    "business_california_sn['is_open'] = pd.to_numeric(business_california_sn['is_open'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardo el archivo parquet, para poder importarlo si es necesario\n",
    "business_california_sn.to_parquet('Archivos/business_CA_Yelp', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review - YELP\n",
    "## Importación de los Datos \n",
    "### para realiza la importacion de los datos de reviews se leyó el archivo por Chunks o bloques de 1000 lineas , luego se hizo un filtrado por el business_id , correspondiente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Definir la ruta al archivo JSON\n",
    "file_path = '/content/drive/My Drive/set_de_datos_Yelp/review (1).json'\n",
    "\n",
    "# Definir el tamaño del chunk (número de líneas por chunk)\n",
    "chunk_size = 1000  # Puedes ajustar este valor según sea necesario\n",
    "\n",
    "# Lista para almacenar los DataFrames procesados\n",
    "results = []\n",
    "\n",
    "# Leer el archivo JSON en chunks\n",
    "chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size)\n",
    "\n",
    "# Procesar cada chunk por separado\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Procesar el chunk como sea necesario\n",
    "    print(f\"Procesando chunk {i + 1}\")\n",
    "\n",
    "    # Ejemplo de procesamiento: mostrar las primeras filas del chunk\n",
    "    print(chunk.head())\n",
    "\n",
    "    # Guardar resultados intermedios en una lista\n",
    "    results.append(chunk)\n",
    "\n",
    "    # Liberar memoria si no necesitas almacenar los chunks completos\n",
    "    del chunk\n",
    "\n",
    "# Combinar todos los chunks procesados en un solo DataFrame final\n",
    "final_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Guardar el DataFrame final en un archivo CSV\n",
    "intermediate_file_path = '/content/drive/My Drive/set_de_datos_Yelp/final_dataframe.csv'\n",
    "final_df.to_csv(intermediate_file_path, index=False)\n",
    "\n",
    "# Liberar memoria después de guardar\n",
    "del final_df\n",
    "del results\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Ahora puedes cargar 'final_dataframe.csv' en cualquier momento para operar sobre él\n",
    "loaded_df = pd.read_csv(intermediate_file_path)\n",
    "\n",
    "# Ejemplo de operaciones sobre el DataFrame cargado\n",
    "print(loaded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lectura del archivo , en formato csv\n",
    "intermediate_file_path = '/content/drive/My Drive/set_de_datos_Yelp/final_dataframe.csv'\n",
    "loaded_df = pd.read_csv(intermediate_file_path)\n",
    "\n",
    "# Ahora puedes cargar 'final_dataframe.csv' en cualquier momento para operar sobre él\n",
    "ruta = '/content/drive/My Drive/set_de_datos_Yelp/business_california.xlsx'\n",
    "business_df = pd.read_excel(ruta)\n",
    "\n",
    "filtered_reviews = loaded_df[loaded_df['business_id'].isin(business_df['business_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular valores nulos y porcentajes\n",
    "valores_nulos = filtered_reviews.isnull().sum()\n",
    "\n",
    "\n",
    "# Contar las ocurrencias de cada fila duplicada\n",
    "duplicate_counts = filtered_reviews[filtered_reviews.duplicated()].sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Escribir el DataFrame como un archivo Parquet\n",
    "df.to_parquet('Archivos/review_ca.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USER - YELP\n",
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lectura del archivo user,parquet\n",
    "df = pd.read_parquet('D:/usuarios/CIN/MIS DOCUMENTOS/Proyecto_Final/set_de_datos_Yelp/user.parquet')\n",
    "# lectura del archivo review_ca.parquet para filtrar solo los de CA y disminuir el tamaño del archivo\n",
    "reviews_ca = pd.read_parquet('Archivos/review_ca.parquet')\n",
    "#Filtrar las reseñas solo para los user_ID específicos del archivo csv\n",
    "filtered_reviews_ca = df[df['user_id'].isin(reviews_ca['user_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular valores nulos y porcentajes\n",
    "valores_nulos = filtered_reviews_ca.isnull().sum()\n",
    "# Contar las ocurrencias de cada fila duplicada\n",
    "duplicate_counts = filtered_reviews_ca[filtered_reviews_ca.duplicated()].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribir el DataFrame como un archivo Parquet\n",
    "filtered_reviews_ca.to_parquet('Archivos/user_ca.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
