{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import gdown\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import ast\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata  - Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar los archivos desde la nube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tamaño:\n",
    "carpeta: 2,76 Gb\n",
    "dataframe 370 Mb\n",
    "csv 2,3 Gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1OnyhmyG8xzdn4XU9LYcnwzBseB1_rChS\n",
      "From (redirected): https://drive.google.com/uc?id=1OnyhmyG8xzdn4XU9LYcnwzBseB1_rChS&confirm=t&uuid=03f8792e-1011-4b2d-8196-607d15726cee\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\1.json\n",
      "100%|██████████| 256M/256M [00:06<00:00, 37.7MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=15D_5EkxqPP0XJb5I5bYI8b1wQV7B2fx_\n",
      "From (redirected): https://drive.google.com/uc?id=15D_5EkxqPP0XJb5I5bYI8b1wQV7B2fx_&confirm=t&uuid=ab4b6a9d-8aee-4153-9cc4-73dc4064ccae\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\2.json\n",
      "100%|██████████| 257M/257M [00:09<00:00, 27.3MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1fDBVCmf4JA7gkIyjpv5mHEMySb19C-vz\n",
      "From (redirected): https://drive.google.com/uc?id=1fDBVCmf4JA7gkIyjpv5mHEMySb19C-vz&confirm=t&uuid=d854ef6a-e45c-47f1-b98a-9a99e26300d0\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\3.json\n",
      "100%|██████████| 259M/259M [00:07<00:00, 33.1MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Mj2oUZy5gGznhthcUGi8_sgKhBwypE74\n",
      "From (redirected): https://drive.google.com/uc?id=1Mj2oUZy5gGznhthcUGi8_sgKhBwypE74&confirm=t&uuid=1067e5be-653b-4575-b702-a927be1436ff\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\4.json\n",
      "100%|██████████| 262M/262M [00:07<00:00, 34.1MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1IXok40Zp61CGwFDgyvLUwV02c4BWGrjj\n",
      "From (redirected): https://drive.google.com/uc?id=1IXok40Zp61CGwFDgyvLUwV02c4BWGrjj&confirm=t&uuid=616a04e7-8512-4e64-a1df-6f680d3a8e7b\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\5.json\n",
      "100%|██████████| 264M/264M [00:06<00:00, 38.4MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1UmsN_ZOFQqVl7W9SbnxHkSQavo1_Iwqx\n",
      "From (redirected): https://drive.google.com/uc?id=1UmsN_ZOFQqVl7W9SbnxHkSQavo1_Iwqx&confirm=t&uuid=1cfd5d8f-386d-4962-8bbc-d5080f9b31d5\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\6.json\n",
      "100%|██████████| 267M/267M [00:09<00:00, 28.9MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1KfQBhJlnuziKjf-9haQGaiPtCPnUUDla\n",
      "From (redirected): https://drive.google.com/uc?id=1KfQBhJlnuziKjf-9haQGaiPtCPnUUDla&confirm=t&uuid=8f8f6dc3-bf3f-47fa-bad8-8ae9d92ac694\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\7.json\n",
      "100%|██████████| 273M/273M [00:11<00:00, 22.9MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1ebTUx2klGy7L9lGlZl3GCPXxSwSD55vX\n",
      "From (redirected): https://drive.google.com/uc?id=1ebTUx2klGy7L9lGlZl3GCPXxSwSD55vX&confirm=t&uuid=e1d23c9f-48ed-41e6-8a41-d812044b1e0a\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\8.json\n",
      "100%|██████████| 284M/284M [00:07<00:00, 38.1MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1td6twU60LAS-z5mB0MeSJEpGhH7jcGKm\n",
      "From (redirected): https://drive.google.com/uc?id=1td6twU60LAS-z5mB0MeSJEpGhH7jcGKm&confirm=t&uuid=e56dec2b-58a8-49f2-8af3-c74e1b225d46\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\9.json\n",
      "100%|██████████| 277M/277M [00:11<00:00, 23.1MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1NQgHgNm9PV8MSiOXNoQ2UkIF9e5AdLk7\n",
      "From (redirected): https://drive.google.com/uc?id=1NQgHgNm9PV8MSiOXNoQ2UkIF9e5AdLk7&confirm=t&uuid=cda88958-c5b3-4495-963e-04d20c30c5e6\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\10.json\n",
      "100%|██████████| 282M/282M [00:08<00:00, 31.9MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1GYwWfH7EvWMZn14vQRNr5CjEely4eWrB\n",
      "From (redirected): https://drive.google.com/uc?id=1GYwWfH7EvWMZn14vQRNr5CjEely4eWrB&confirm=t&uuid=a03baabd-a7c2-4d54-807d-febc9e8e1522\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\11.json\n",
      "100%|██████████| 288M/288M [00:09<00:00, 30.7MB/s] \n"
     ]
    }
   ],
   "source": [
    "## descargados directamente desde google drive\n",
    "\n",
    "# IDs de los archivos de Google Drive\n",
    "file_links = [\n",
    "    'https://drive.google.com/file/d/1OnyhmyG8xzdn4XU9LYcnwzBseB1_rChS', \n",
    "    'https://drive.google.com/file/d/15D_5EkxqPP0XJb5I5bYI8b1wQV7B2fx_', \n",
    "    'https://drive.google.com/file/d/1fDBVCmf4JA7gkIyjpv5mHEMySb19C-vz', \n",
    "    'https://drive.google.com/file/d/1Mj2oUZy5gGznhthcUGi8_sgKhBwypE74', \n",
    "    'https://drive.google.com/file/d/1IXok40Zp61CGwFDgyvLUwV02c4BWGrjj',\n",
    "    'https://drive.google.com/file/d/1UmsN_ZOFQqVl7W9SbnxHkSQavo1_Iwqx', \n",
    "    'https://drive.google.com/file/d/1KfQBhJlnuziKjf-9haQGaiPtCPnUUDla', \n",
    "    'https://drive.google.com/file/d/1ebTUx2klGy7L9lGlZl3GCPXxSwSD55vX', \n",
    "    'https://drive.google.com/file/d/1td6twU60LAS-z5mB0MeSJEpGhH7jcGKm', \n",
    "    'https://drive.google.com/file/d/1NQgHgNm9PV8MSiOXNoQ2UkIF9e5AdLk7', \n",
    "    'https://drive.google.com/file/d/1GYwWfH7EvWMZn14vQRNr5CjEely4eWrB'\n",
    "]\n",
    "\n",
    "# Nombres de los archivos locales (presumiendo que siguen el patrón 1.json, 2.json, ..., 11.json)\n",
    "file_names = [f'{i}.json' for i in range(1, len(file_links) + 1)]\n",
    "\n",
    "# Inicializar una lista para almacenar los DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Descargar y leer cada archivo JSON\n",
    "for file_link, file_name in zip(file_links, file_names):\n",
    "    try:\n",
    "        # Obtener el ID del archivo desde el enlace\n",
    "        file_id = file_link.split('/d/')[1].split('/')[0]\n",
    "        download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "        \n",
    "        # Descargar el archivo\n",
    "        gdown.download(download_url, file_name, quiet=False)\n",
    "        \n",
    "        # Verificar si el archivo descargado es un JSON válido\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            first_char = file.read(1)\n",
    "            if not first_char:\n",
    "                raise ValueError(f\"El archivo {file_name} está vacío.\")\n",
    "            file.seek(0)\n",
    "        \n",
    "        # Leer el archivo JSON en un DataFrame\n",
    "        df = pd.read_json(file_name, lines=True)\n",
    "        \n",
    "        # Agregar el DataFrame a la lista\n",
    "        dataframes.append(df)\n",
    "        \n",
    "        # Eliminar el archivo descargado\n",
    "        os.remove(file_name)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el archivo {file_name}: {e}\")\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_metadataGoog = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrar por estado California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraer el estado de la dirección\n",
    "df_metadataGoog['estado'] = df_metadataGoog['address'].str.extract(r', ([A-Z]{2}) \\d{5}')\n",
    "#filtro los datos de California, para liberar espacio\n",
    "df_metadatosCA = df_metadataGoog[df_metadataGoog['estado'] == 'CA']\n",
    "#elimino el dataframe que tiene los metadatos de todos los estados, para liberar memoria\n",
    "del df_metadataGoog\n",
    "#reseteo indice\n",
    "df_metadatosCA.reset_index(inplace=True)\n",
    "df_metadatosCA.drop('index', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armado de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solo para usar cuando se quiere importar el archivo\n",
    "#df_metadatosCA=pd.read_parquet('Archivos/metadatosCA.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer Ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer la ciudad\n",
    "def extract_city(address):\n",
    "    match = re.search(r',\\s*([^,]+),\\s*[A-Z]{2}\\s*\\d{5}', address)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "# Aplicar la función a la columna 'address' y crear la columna 'city'\n",
    "df_metadatosCA['city'] = df_metadatosCA['address'].apply(extract_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para asegurarse de que el tiempo esté en el formato \"HH:MMAM/PM\"\n",
    "def ensure_time_format(time_str):\n",
    "    if '–' in time_str:\n",
    "        parts = time_str.split('–')\n",
    "        parts = [ensure_time_format(part) for part in parts]\n",
    "        return '–'.join(parts)\n",
    "    try:\n",
    "        if ':' not in time_str:\n",
    "            time_obj = pd.to_datetime(time_str, format='%I%p', errors='coerce')\n",
    "        else:\n",
    "            time_obj = pd.to_datetime(time_str, format='%I:%M%p', errors='coerce')\n",
    "        if time_obj is not pd.NaT:\n",
    "            return time_obj.strftime('%I:%M%p')\n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting time: {time_str}, error: {e}\")\n",
    "    return None\n",
    "\n",
    "# Función para calcular las horas diurnas (8 AM - 10 PM)\n",
    "def calculate_day_hours(hours_array):\n",
    "    total_day_hours = 0\n",
    "    if hours_array is not None and isinstance(hours_array, np.ndarray):\n",
    "        for entry in hours_array:\n",
    "            if isinstance(entry, np.ndarray) and len(entry) == 2:\n",
    "                day, hours = entry\n",
    "                if 'Closed' in hours:\n",
    "                    continue\n",
    "                if '–' in hours:\n",
    "                    open_time, close_time = hours.split('–')\n",
    "                    open_time = ensure_time_format(open_time)\n",
    "                    close_time = ensure_time_format(close_time)\n",
    "                    \n",
    "                    if open_time and close_time:\n",
    "                        open_hour = pd.to_datetime(open_time, format='%I:%M%p').hour\n",
    "                        close_hour = pd.to_datetime(close_time, format='%I:%M%p').hour\n",
    "\n",
    "                        if open_hour < 8:\n",
    "                            open_hour = 8\n",
    "                        if close_hour > 22:\n",
    "                            close_hour = 22\n",
    "\n",
    "                        if close_hour < open_hour:\n",
    "                            close_hour += 24  # Para manejar el cambio de día\n",
    "\n",
    "                        total_day_hours += max(0, close_hour - open_hour)\n",
    "    return total_day_hours\n",
    "\n",
    "# Función para calcular las horas nocturnas (10 PM - 8 AM)\n",
    "def calculate_night_hours(hours_array):\n",
    "    total_night_hours = 0\n",
    "    if hours_array is not None and isinstance(hours_array, np.ndarray):\n",
    "        for entry in hours_array:\n",
    "            if isinstance(entry, np.ndarray) and len(entry) == 2:\n",
    "                day, hours = entry\n",
    "                if 'Closed' in hours:\n",
    "                    continue\n",
    "                if '–' in hours:\n",
    "                    open_time, close_time = hours.split('–')\n",
    "                    open_time = ensure_time_format(open_time)\n",
    "                    close_time = ensure_time_format(close_time)\n",
    "\n",
    "                    if open_time and close_time:\n",
    "                        open_hour = pd.to_datetime(open_time, format='%I:%M%p').hour\n",
    "                        close_hour = pd.to_datetime(close_time, format='%I:%M%p').hour\n",
    "\n",
    "                        if close_hour < open_hour:\n",
    "                            close_hour += 24  # Para manejar el cambio de día\n",
    "\n",
    "                        night_hours = 0\n",
    "                        if open_hour < 8:\n",
    "                            night_hours += min(8, close_hour) - open_hour\n",
    "                        if close_hour > 22:\n",
    "                            night_hours += close_hour - 22\n",
    "\n",
    "                        total_night_hours += max(0, night_hours)\n",
    "    return total_night_hours\n",
    "\n",
    "# Aplicar las funciones al DataFrame\n",
    "df_metadatosCA['Hours_day'] = df_metadatosCA['hours'].apply(calculate_day_hours)\n",
    "df_metadatosCA['Hours_night'] = df_metadatosCA['hours'].apply(calculate_night_hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expandir las listas en filas individuales\n",
    "categorias_expandidas = df_metadatosCA['category'].explode()\n",
    "\n",
    "# Contar las ocurrencias de cada categoría\n",
    "conteo_categorias = categorias_expandidas.value_counts()\n",
    "\n",
    "# Eliminar la serie que ya no se usa\n",
    "del categorias_expandidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo_categorias.to_csv('Archivos/categoriasCA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explotar columna MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer y expandir los diccionarios en nuevas columnas\n",
    "def expand_misc_column(misc_dict):\n",
    "    if pd.isna(misc_dict):\n",
    "        return pd.Series()\n",
    "    expanded = {}\n",
    "    for key, value in misc_dict.items():\n",
    "        if value is not None and isinstance(value, np.ndarray):\n",
    "            expanded[key] = ', '.join(value)\n",
    "        else:\n",
    "            expanded[key] = value\n",
    "    return pd.Series(expanded)\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "expanded_df = df_metadatosCA['MISC'].apply(expand_misc_column)\n",
    "\n",
    "# Unir el DataFrame original con el DataFrame expandido\n",
    "df_metadatosCA = pd.concat([df_metadatosCA, expanded_df], axis=1)\n",
    "\n",
    "#Eliminar la columna MISC que ya no se usa\n",
    "df_metadatosCA.drop(columns='MISC', inplace=True)\n",
    "\n",
    "del expanded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accesibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "          ...  \n",
       "297288    False\n",
       "297289    False\n",
       "297290    False\n",
       "297291    False\n",
       "297292    False\n",
       "Name: Accessibility, Length: 297293, dtype: bool"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadatosCA['Accessibility']=='Wheelchair accessible entrance, Wheelchair accessible parking lot, Wheelchair accessible restroom, Wheelchair accessible seating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadatosCA['Activities'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'address', 'gmap_id', 'description', 'latitude', 'longitude',\n",
       "       'category', 'avg_rating', 'num_of_reviews', 'price', 'hours', 'state',\n",
       "       'relative_results', 'url', 'estado', 'city', 'Hours_day', 'Hours_night',\n",
       "       'Accessibility', 'Activities', 'Amenities', 'Atmosphere', 'Crowd',\n",
       "       'Dining options', 'From the business', 'Getting here',\n",
       "       'Health & safety', 'Health and safety', 'Highlights', 'Offerings',\n",
       "       'Payments', 'Planning', 'Popular for', 'Recycling', 'Service options'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadatosCA['Accessibility'].value_counts()\n",
    "df_metadatosCA['Activities'].value_counts()\n",
    "df_metadatosCA['Amenities'].value_counts()\n",
    "df_metadatosCA['Atmosphere'].value_counts()\n",
    "df_metadatosCA['Crowd'].value_counts()\n",
    "df_metadatosCA['Dining options'].value_counts()\n",
    "df_metadatosCA['From the business'].value_counts()\n",
    "df_metadatosCA['Getting here'].value_counts()\n",
    "df_metadatosCA['Health & safety'].value_counts()\n",
    "df_metadatosCA['Health and safety'].value_counts()\n",
    "df_metadatosCA['Highlights'].value_counts()\n",
    "df_metadatosCA['Offerings'].value_counts()\n",
    "df_metadatosCA['Payments'].value_counts()\n",
    "df_metadatosCA['Planning'].value_counts()\n",
    "df_metadatosCA['Popular for'].value_counts()\n",
    "df_metadatosCA['Recycling'].value_counts()\n",
    "df_metadatosCA['Service options'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegir ciudad\n",
    "ciudadelegida='Los Angeles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar el dataframe a exportar\n",
    "df_ML = df_metadatosCA.loc[df_metadatosCA['city'] == ciudadelegida, \n",
    "                           ['address', 'gmap_id', 'latitude', 'longitude',\n",
    "                            'category', 'avg_rating', 'num_of_reviews', 'Hours_day', 'Hours_night']]\n",
    "\n",
    "# Exportar el dataframe\n",
    "df_ML.to_csv('Archivos/metadatos_ML.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardo el archivo parquet, para poder importarlo si es necesario\n",
    "df_metadatosCA.to_parquet('Archivos/metadatosCA.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews Estados - Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar datos (cambiar a la nube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo y almacenar cada línea en una lista\n",
    "lines = []\n",
    "\n",
    "for i in range (18):\n",
    "    file='reviews-estados/review-California/'+str(i+1)+'.json'\n",
    "    with open(file, 'r') as file:\n",
    "        for line in file:\n",
    "            lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir todas las líneas en un solo string y leerlo como un DataFrame\n",
    "all_lines = ''.join(lines)\n",
    "df_reviews = pd.read_json(StringIO(all_lines), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lines\n",
    "del all_lines\n",
    "del file\n",
    "del line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Espacio que ocupan\n",
    "Carpeta: 763 Mb (3,8 segundos en importar para una lista, 48,44 segundos para pasarlo a dataframe)\n",
    "Dataframe: 164,8 Mb\n",
    "CSV: 545 Mbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business - YELP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar los archivos desde la nube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1byFtzpZXopdCN-XYmMHMpZqzgAqfQBBu\n",
      "From (redirected): https://drive.google.com/uc?id=1byFtzpZXopdCN-XYmMHMpZqzgAqfQBBu&confirm=t&uuid=69cf883e-c12b-4736-b320-965493d099b8\n",
      "To: c:\\Users\\feder\\Desktop\\PF\\business.pkl\n",
      "100%|██████████| 116M/116M [00:03<00:00, 36.9MB/s] \n"
     ]
    }
   ],
   "source": [
    "# ID del archivo de Google Drive\n",
    "file_id = '1byFtzpZXopdCN-XYmMHMpZqzgAqfQBBu'\n",
    "# Construir la URL de descarga\n",
    "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "# Descargar el archivo usando gdown\n",
    "file_path = 'business.pkl'\n",
    "gdown.download(download_url, file_path, quiet=False)\n",
    "\n",
    "# Cargar el archivo pickle en un DataFrame de pandas\n",
    "df_business = pd.read_pickle(file_path)\n",
    "\n",
    "# Eliminar el archivo descargado\n",
    "os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de los Datos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina las columnas duplicadas\n",
    "df = df_business.loc[:, ~df_business.columns.duplicated()]\n",
    "\n",
    "#  Filtra negocios en un estado específico (por ejemplo, 'CA' para California)\n",
    "business_california = df[df['state'] == 'CA']\n",
    "\n",
    "# Elimina valores nulos en las columnas attributes y hours\n",
    "business_california_sn= business_california.dropna(subset=['attributes', 'hours'])\n",
    "\n",
    "# Modificación del tipo  de datos en las columnas 'latitude','longitud','stars','review_count','is-open'\n",
    "\n",
    "business_california_sn['latitude'] = pd.to_numeric(business_california_sn['latitude'], errors='coerce')\n",
    "business_california_sn['longitude'] = pd.to_numeric(business_california_sn['longitude'], errors='coerce')\n",
    "business_california_sn['stars'] = pd.to_numeric(business_california_sn['stars'], errors='coerce')\n",
    "business_california_sn['review_count'] = pd.to_numeric(business_california_sn['review_count'], errors='coerce')\n",
    "business_california_sn['is_open'] = pd.to_numeric(business_california_sn['is_open'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardo el archivo parquet, para poder importarlo si es necesario\n",
    "business_california_sn.to_parquet('Archivos/business_CA_Yelp', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review - YELP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de los Datos \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para realiza la importacion de los datos de reviews se leyó el archivo por Chunks o bloques de 1000 lineas , luego se hizo un filtrado por el business_id , correspondiente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la ruta al archivo JSON\n",
    "file_path = '/content/drive/My Drive/set_de_datos_Yelp/review (1).json'\n",
    "\n",
    "# Definir el tamaño del chunk (número de líneas por chunk)\n",
    "chunk_size = 1000  # Puedes ajustar este valor según sea necesario\n",
    "\n",
    "# Lista para almacenar los DataFrames procesados\n",
    "results = []\n",
    "\n",
    "# Leer el archivo JSON en chunks\n",
    "chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size)\n",
    "\n",
    "# Procesar cada chunk por separado\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Procesar el chunk como sea necesario\n",
    "    print(f\"Procesando chunk {i + 1}\")\n",
    "\n",
    "    # Ejemplo de procesamiento: mostrar las primeras filas del chunk\n",
    "    print(chunk.head())\n",
    "\n",
    "    # Guardar resultados intermedios en una lista\n",
    "    results.append(chunk)\n",
    "\n",
    "    # Liberar memoria si no necesitas almacenar los chunks completos\n",
    "    del chunk\n",
    "\n",
    "# Combinar todos los chunks procesados en un solo DataFrame final\n",
    "final_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Guardar el DataFrame final en un archivo CSV\n",
    "intermediate_file_path = '/content/drive/My Drive/set_de_datos_Yelp/final_dataframe.csv'\n",
    "final_df.to_csv(intermediate_file_path, index=False)\n",
    "\n",
    "# Liberar memoria después de guardar\n",
    "del final_df\n",
    "del results\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Ahora puedes cargar 'final_dataframe.csv' en cualquier momento para operar sobre él\n",
    "loaded_df = pd.read_csv(intermediate_file_path)\n",
    "\n",
    "# Ejemplo de operaciones sobre el DataFrame cargado\n",
    "print(loaded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lectura del archivo , en formato csv\n",
    "intermediate_file_path = '/content/drive/My Drive/set_de_datos_Yelp/final_dataframe.csv'\n",
    "loaded_df = pd.read_csv(intermediate_file_path)\n",
    "\n",
    "# Ahora puedes cargar 'final_dataframe.csv' en cualquier momento para operar sobre él\n",
    "ruta = '/content/drive/My Drive/set_de_datos_Yelp/business_california.xlsx'\n",
    "business_df = pd.read_excel(ruta)\n",
    "\n",
    "filtered_reviews = loaded_df[loaded_df['business_id'].isin(business_df['business_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular valores nulos y porcentajes\n",
    "valores_nulos = filtered_reviews.isnull().sum()\n",
    "\n",
    "\n",
    "# Contar las ocurrencias de cada fila duplicada\n",
    "duplicate_counts = filtered_reviews[filtered_reviews.duplicated()].sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Escribir el DataFrame como un archivo Parquet\n",
    "df.to_parquet('Archivos/review_ca.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USER - YELP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lectura del archivo user,parquet\n",
    "df = pd.read_parquet('D:/usuarios/CIN/MIS DOCUMENTOS/Proyecto_Final/set_de_datos_Yelp/user.parquet')\n",
    "# lectura del archivo review_ca.parquet para filtrar solo los de CA y disminuir el tamaño del archivo\n",
    "reviews_ca = pd.read_parquet('Archivos/review_ca.parquet')\n",
    "#Filtrar las reseñas solo para los user_ID específicos del archivo csv\n",
    "filtered_reviews_ca = df[df['user_id'].isin(reviews_ca['user_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular valores nulos y porcentajes\n",
    "valores_nulos = filtered_reviews_ca.isnull().sum()\n",
    "# Contar las ocurrencias de cada fila duplicada\n",
    "duplicate_counts = filtered_reviews_ca[filtered_reviews_ca.duplicated()].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribir el DataFrame como un archivo Parquet\n",
    "filtered_reviews_ca.to_parquet('Archivos/user_ca.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
