{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import gdown\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import ast\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata  - Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar los archivos desde la GoogleDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tamaño:\n",
    "carpeta: 2,76 Gb\n",
    "dataframe 370 Mb\n",
    "csv 2,3 Gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IDs de los archivos de Google Drive\n",
    "file_links = [\n",
    "    'https://drive.google.com/file/d/1OnyhmyG8xzdn4XU9LYcnwzBseB1_rChS', \n",
    "    'https://drive.google.com/file/d/15D_5EkxqPP0XJb5I5bYI8b1wQV7B2fx_', \n",
    "    'https://drive.google.com/file/d/1fDBVCmf4JA7gkIyjpv5mHEMySb19C-vz', \n",
    "    'https://drive.google.com/file/d/1Mj2oUZy5gGznhthcUGi8_sgKhBwypE74', \n",
    "    'https://drive.google.com/file/d/1IXok40Zp61CGwFDgyvLUwV02c4BWGrjj',\n",
    "    'https://drive.google.com/file/d/1UmsN_ZOFQqVl7W9SbnxHkSQavo1_Iwqx', \n",
    "    'https://drive.google.com/file/d/1KfQBhJlnuziKjf-9haQGaiPtCPnUUDla', \n",
    "    'https://drive.google.com/file/d/1ebTUx2klGy7L9lGlZl3GCPXxSwSD55vX', \n",
    "    'https://drive.google.com/file/d/1td6twU60LAS-z5mB0MeSJEpGhH7jcGKm', \n",
    "    'https://drive.google.com/file/d/1NQgHgNm9PV8MSiOXNoQ2UkIF9e5AdLk7', \n",
    "    'https://drive.google.com/file/d/1GYwWfH7EvWMZn14vQRNr5CjEely4eWrB'\n",
    "]\n",
    "\n",
    "# Nombres de los archivos locales (presumiendo que siguen el patrón 1.json, 2.json, ..., 11.json)\n",
    "file_names = [f'{i}.json' for i in range(1, len(file_links) + 1)]\n",
    "\n",
    "# Crear la carpeta ArchivosIgnore si no existe\n",
    "output_dir = 'MetadataGoogle'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Inicializar una lista para almacenar los DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Descargar y leer cada archivo JSON\n",
    "for file_link, file_name in zip(file_links, file_names):\n",
    "    try:\n",
    "        # Ruta completa del archivo\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        \n",
    "        # Verificar si el archivo ya está descargado\n",
    "        if not os.path.exists(file_path):\n",
    "            # Obtener el ID del archivo desde el enlace\n",
    "            file_id = file_link.split('/d/')[1].split('/')[0]\n",
    "            download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "            \n",
    "            # Descargar el archivo\n",
    "            gdown.download(download_url, file_path, quiet=False)\n",
    "        \n",
    "        # Leer el archivo JSON en un DataFrame\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            first_char = file.read(1)\n",
    "            if not first_char:\n",
    "                raise ValueError(f\"El archivo {file_name} está vacío.\")\n",
    "            file.seek(0)\n",
    "            df = pd.read_json(file, lines=True)\n",
    "        \n",
    "        # Agregar el DataFrame a la lista\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el archivo {file_name}: {e}\")\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_metadataGoogle = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrar por estado California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraer el estado de la dirección\n",
    "df_metadataGoogle['estado'] = df_metadataGoogle['address'].str.extract(r', ([A-Z]{2}) \\d{5}')\n",
    "#filtro los datos de California, para liberar espacio\n",
    "df_metadatosCA = df_metadataGoogle[df_metadataGoogle['estado'] == 'CA']\n",
    "#elimino el dataframe que tiene los metadatos de todos los estados, para liberar memoria\n",
    "del df_metadataGoogle\n",
    "#reseteo indice\n",
    "df_metadatosCA.reset_index(inplace=True)\n",
    "df_metadatosCA.drop('index', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadatosCA.drop_duplicates(subset=['gmap_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armado de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solo para usar cuando se quiere importar el archivo\n",
    "#df_metadatosCA=pd.read_parquet('Archivos/metadatosCA.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer Ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer la ciudad\n",
    "def extract_city(address):\n",
    "    match = re.search(r',\\s*([^,]+),\\s*[A-Z]{2}\\s*\\d{5}', address)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "# Aplicar la función a la columna 'address' y crear la columna 'city'\n",
    "df_metadatosCA['city'] = df_metadatosCA['address'].apply(extract_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para asegurarse de que el tiempo esté en el formato \"HH:MMAM/PM\"\n",
    "def ensure_time_format(time_str):\n",
    "    if '–' in time_str:\n",
    "        parts = time_str.split('–')\n",
    "        parts = [ensure_time_format(part) for part in parts]\n",
    "        return '–'.join(parts)\n",
    "    try:\n",
    "        if ':' not in time_str:\n",
    "            time_obj = pd.to_datetime(time_str, format='%I%p', errors='coerce')\n",
    "        else:\n",
    "            time_obj = pd.to_datetime(time_str, format='%I:%M%p', errors='coerce')\n",
    "        if time_obj is not pd.NaT:\n",
    "            return time_obj.strftime('%I:%M%p')\n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting time: {time_str}, error: {e}\")\n",
    "    return None\n",
    "\n",
    "# Función para calcular las horas diurnas (8 AM - 10 PM)\n",
    "def calculate_day_hours(hours_array):\n",
    "    total_day_hours = 0\n",
    "    if hours_array is not None and isinstance(hours_array, np.ndarray):\n",
    "        for entry in hours_array:\n",
    "            if isinstance(entry, np.ndarray) and len(entry) == 2:\n",
    "                day, hours = entry\n",
    "                if 'Closed' in hours:\n",
    "                    continue\n",
    "                if '–' in hours:\n",
    "                    open_time, close_time = hours.split('–')\n",
    "                    open_time = ensure_time_format(open_time)\n",
    "                    close_time = ensure_time_format(close_time)\n",
    "                    \n",
    "                    if open_time and close_time:\n",
    "                        open_hour = pd.to_datetime(open_time, format='%I:%M%p').hour\n",
    "                        close_hour = pd.to_datetime(close_time, format='%I:%M%p').hour\n",
    "\n",
    "                        if open_hour < 8:\n",
    "                            open_hour = 8\n",
    "                        if close_hour > 22:\n",
    "                            close_hour = 22\n",
    "\n",
    "                        if close_hour < open_hour:\n",
    "                            close_hour += 24  # Para manejar el cambio de día\n",
    "\n",
    "                        total_day_hours += max(0, close_hour - open_hour)\n",
    "    return total_day_hours\n",
    "\n",
    "# Función para calcular las horas nocturnas (10 PM - 8 AM)\n",
    "def calculate_night_hours(hours_array):\n",
    "    total_night_hours = 0\n",
    "    if hours_array is not None and isinstance(hours_array, np.ndarray):\n",
    "        for entry in hours_array:\n",
    "            if isinstance(entry, np.ndarray) and len(entry) == 2:\n",
    "                day, hours = entry\n",
    "                if 'Closed' in hours:\n",
    "                    continue\n",
    "                if '–' in hours:\n",
    "                    open_time, close_time = hours.split('–')\n",
    "                    open_time = ensure_time_format(open_time)\n",
    "                    close_time = ensure_time_format(close_time)\n",
    "\n",
    "                    if open_time and close_time:\n",
    "                        open_hour = pd.to_datetime(open_time, format='%I:%M%p').hour\n",
    "                        close_hour = pd.to_datetime(close_time, format='%I:%M%p').hour\n",
    "\n",
    "                        if close_hour < open_hour:\n",
    "                            close_hour += 24  # Para manejar el cambio de día\n",
    "\n",
    "                        night_hours = 0\n",
    "                        if open_hour < 8:\n",
    "                            night_hours += min(8, close_hour) - open_hour\n",
    "                        if close_hour > 22:\n",
    "                            night_hours += close_hour - 22\n",
    "\n",
    "                        total_night_hours += max(0, night_hours)\n",
    "    return total_night_hours\n",
    "\n",
    "# Aplicar las funciones al DataFrame\n",
    "df_metadatosCA['Hours_day'] = df_metadatosCA['hours'].apply(calculate_day_hours)\n",
    "df_metadatosCA['Hours_night'] = df_metadatosCA['hours'].apply(calculate_night_hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expandir las listas en filas individuales\n",
    "categorias_expandidas = df_metadatosCA['category'].explode()\n",
    "\n",
    "# Contar las ocurrencias de cada categoría\n",
    "conteo_categorias = categorias_expandidas.value_counts()\n",
    "\n",
    "# Eliminar la serie que ya no se usa\n",
    "del categorias_expandidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica si la carpeta 'Archivos' existe, si no, la crea\n",
    "if not os.path.exists('Archivos'):\n",
    "    os.makedirs('Archivos')\n",
    "\n",
    "# Guarda el DataFrame en un archivo CSV en la carpeta 'Archivos'\n",
    "conteo_categorias.to_csv('Archivos/categoriasCA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explotar columna MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer y expandir los diccionarios en nuevas columnas\n",
    "def expand_misc_column(misc_dict):\n",
    "    if pd.isna(misc_dict):\n",
    "        return pd.Series()\n",
    "    expanded = {}\n",
    "    for key, value in misc_dict.items():\n",
    "        if value is not None and isinstance(value, np.ndarray):\n",
    "            expanded[key] = ', '.join(value)\n",
    "        else:\n",
    "            expanded[key] = value\n",
    "    return pd.Series(expanded)\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "expanded_df = df_metadatosCA['MISC'].apply(expand_misc_column)\n",
    "\n",
    "# Unir el DataFrame original con el DataFrame expandido\n",
    "df_metadatosCA = pd.concat([df_metadatosCA, expanded_df], axis=1)\n",
    "\n",
    "#Eliminar la columna MISC que ya no se usa\n",
    "df_metadatosCA.drop(columns='MISC', inplace=True)\n",
    "\n",
    "del expanded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegir ciudad\n",
    "ciudadelegida='Los Angeles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listado de negocios de la ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtro el listado\n",
    "negocios=df_metadatosCA['gmap_id'][df_metadatosCA['city'] == ciudadelegida]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lo exporto a un csv\n",
    "negocios.to_csv('Archivos/negociosciudad.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegir ciudad\n",
    "ciudadelegida='Los Angeles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar el dataframe a exportar\n",
    "df_ML = df_metadatosCA.loc[df_metadatosCA['city'] == ciudadelegida, \n",
    "                           ['address', 'gmap_id', 'latitude', 'longitude',\n",
    "                            'category', 'avg_rating', 'num_of_reviews', 'Hours_day', 'Hours_night']]\n",
    "\n",
    "# Exportar el dataframe\n",
    "df_ML.to_csv('Archivos/metadatos_ML.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadatosCA['Service options'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadatosCA.drop(columns=['name', 'description', 'hours', 'state', 'relative_results', 'From the business'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardo el archivo parquet, para poder importarlo si es necesario\n",
    "df_metadatosCA.to_parquet('Archivos/metadatosCA.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews Estados - Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha creado la carpeta: ReviewsEstadosGoogle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=13JlGdagtTp4SrUIXu5osayX0f-vmeMz6\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\1.json\n",
      "100%|██████████| 47.9M/47.9M [00:21<00:00, 2.22MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1PIruhKSA5gEwk93-jwKdlG_vtwJmBHWV\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\2.json\n",
      "100%|██████████| 47.3M/47.3M [00:22<00:00, 2.08MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1JVSi-345m8nt52m2_MPkLULZexbvZUAV\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\3.json\n",
      "100%|██████████| 46.9M/46.9M [00:23<00:00, 2.02MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1vYYCtcNcfdRzQpEskb8x-8npZUHj2XY-\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\4.json\n",
      "100%|██████████| 46.4M/46.4M [00:26<00:00, 1.72MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1nCyVnhNpfphd26ye3lj9UsWvPTEhik6b\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\5.json\n",
      "100%|██████████| 46.7M/46.7M [00:19<00:00, 2.44MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=12PR9jUiZLYvjw6BZjwlexgsWmJlMOajN\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\6.json\n",
      "100%|██████████| 45.9M/45.9M [00:26<00:00, 1.72MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Oq1UdTmQ4xFgkaFdx09JXJQ1_pnIk-il\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\7.json\n",
      "100%|██████████| 46.4M/46.4M [00:18<00:00, 2.47MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1UwzEftWrssj8Vt0BAf_W9L_TVDGa9JD9\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\8.json\n",
      "100%|██████████| 46.4M/46.4M [00:32<00:00, 1.44MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1KsXni6or_cPKovgUaRI_3G4-mRXfqgog\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\9.json\n",
      "100%|██████████| 44.0M/44.0M [00:20<00:00, 2.12MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1fK2kTLDqlUcDt6bKa20W5LvOmrfemDrg\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\10.json\n",
      "100%|██████████| 43.4M/43.4M [00:17<00:00, 2.53MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1rMz_y1cqa8IBwv1K6K34fAXB2qoRjmEG\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\11.json\n",
      "100%|██████████| 42.8M/42.8M [00:22<00:00, 1.95MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1t59IfitryIsy8-F9NL9J6M75UElQO0i9\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\12.json\n",
      "100%|██████████| 42.5M/42.5M [00:17<00:00, 2.40MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=17VtmF8701j3Tk-tdHeRrXDzj32UyWFVh\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\13.json\n",
      "100%|██████████| 43.2M/43.2M [00:17<00:00, 2.52MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1zoN6XJV220ofRKVlM8DP--FriL_OcZEP\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\14.json\n",
      "100%|██████████| 42.8M/42.8M [00:16<00:00, 2.64MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1HUVCM9uOrXhoOzoSD9NqhHJ1PHrLEhBC\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\15.json\n",
      "100%|██████████| 43.0M/43.0M [00:19<00:00, 2.19MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1sqp0YG4OHVUoA0gWrgwg1wXpdOlF5RfD\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\16.json\n",
      "100%|██████████| 40.8M/40.8M [00:18<00:00, 2.18MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1SPDbJPTxKV1QqMcRNsHIhV7EZBLtRCWf\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\17.json\n",
      "100%|██████████| 42.2M/42.2M [00:37<00:00, 1.13MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1_Ik1uLilfLe0MEb1Gia-t9SpE1Wwdwnm\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewsEstadosGoogle\\18.json\n",
      "100%|██████████| 41.5M/41.5M [00:27<00:00, 1.49MB/s]\n"
     ]
    }
   ],
   "source": [
    "# IDs de los archivos de Google Drive\n",
    "file_links = [\n",
    "    'https://drive.google.com/file/d/13JlGdagtTp4SrUIXu5osayX0f-vmeMz6',\n",
    "    'https://drive.google.com/file/d/1PIruhKSA5gEwk93-jwKdlG_vtwJmBHWV', \n",
    "    'https://drive.google.com/file/d/1JVSi-345m8nt52m2_MPkLULZexbvZUAV', \n",
    "    'https://drive.google.com/file/d/1vYYCtcNcfdRzQpEskb8x-8npZUHj2XY-', \n",
    "    'https://drive.google.com/file/d/1nCyVnhNpfphd26ye3lj9UsWvPTEhik6b', \n",
    "    'https://drive.google.com/file/d/12PR9jUiZLYvjw6BZjwlexgsWmJlMOajN',\n",
    "    'https://drive.google.com/file/d/1Oq1UdTmQ4xFgkaFdx09JXJQ1_pnIk-il', \n",
    "    'https://drive.google.com/file/d/1UwzEftWrssj8Vt0BAf_W9L_TVDGa9JD9', \n",
    "    'https://drive.google.com/file/d/1KsXni6or_cPKovgUaRI_3G4-mRXfqgog', \n",
    "    'https://drive.google.com/file/d/1fK2kTLDqlUcDt6bKa20W5LvOmrfemDrg', \n",
    "    'https://drive.google.com/file/d/1rMz_y1cqa8IBwv1K6K34fAXB2qoRjmEG', \n",
    "    'https://drive.google.com/file/d/1t59IfitryIsy8-F9NL9J6M75UElQO0i9',\n",
    "    'https://drive.google.com/file/d/17VtmF8701j3Tk-tdHeRrXDzj32UyWFVh',\n",
    "    'https://drive.google.com/file/d/1zoN6XJV220ofRKVlM8DP--FriL_OcZEP',\n",
    "    'https://drive.google.com/file/d/1HUVCM9uOrXhoOzoSD9NqhHJ1PHrLEhBC',\n",
    "    'https://drive.google.com/file/d/1sqp0YG4OHVUoA0gWrgwg1wXpdOlF5RfD',\n",
    "    'https://drive.google.com/file/d/1SPDbJPTxKV1QqMcRNsHIhV7EZBLtRCWf',\n",
    "    'https://drive.google.com/file/d/1_Ik1uLilfLe0MEb1Gia-t9SpE1Wwdwnm'\n",
    "]\n",
    "\n",
    "# Nombre de la carpeta que quieres crear\n",
    "carpeta_destino = 'ReviewsEstadosGoogle'\n",
    "\n",
    "# Verificar si la carpeta existe\n",
    "if not os.path.exists(carpeta_destino):\n",
    "    # Crear la carpeta si no existe\n",
    "    os.makedirs(carpeta_destino)\n",
    "    print(f\"Se ha creado la carpeta: {carpeta_destino}\")\n",
    "else:\n",
    "    print(f\"La carpeta {carpeta_destino} ya existe.\")\n",
    "    \n",
    "# Nombres de los archivos locales (presumiendo que siguen el patrón 1.json, 2.json, ..., 11.json)\n",
    "file_names = [f'ReviewsEstadosGoogle/{i}.json' for i in range(1, len(file_links) + 1)]\n",
    "\n",
    "# Inicializar una lista para almacenar los DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Descargar y leer cada archivo JSON si no está descargado previamente\n",
    "for file_link, file_name in zip(file_links, file_names):\n",
    "    try:\n",
    "        # Verificar si el archivo ya está descargado\n",
    "        if not os.path.exists(file_name):\n",
    "            # Obtener el ID del archivo desde el enlace\n",
    "            file_id = file_link.split('/d/')[1].split('/')[0]\n",
    "            download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "            \n",
    "            # Descargar el archivo\n",
    "            gdown.download(download_url, file_name, quiet=False)\n",
    "            \n",
    "            # Verificar si el archivo descargado es un JSON válido\n",
    "            with open(file_name, 'r', encoding='utf-8') as file:\n",
    "                first_char = file.read(1)\n",
    "                if not first_char:\n",
    "                    raise ValueError(f\"El archivo {file_name} está vacío.\")\n",
    "                file.seek(0)\n",
    "            \n",
    "        # Leer el archivo JSON en un DataFrame\n",
    "        df = pd.read_json(file_name, lines=True)\n",
    "        \n",
    "        # Agregar el DataFrame a la lista\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el archivo {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_reviewsGoogle = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del file_link\n",
    "del file_name\n",
    "del file_names\n",
    "del file_links\n",
    "del first_char\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Espacio que ocupan\n",
    "Carpeta: 763 Mb (3,8 segundos en importar para una lista, 48,44 segundos para pasarlo a dataframe)\n",
    "Dataframe: 164,8 Mb\n",
    "CSV: 545 Mbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminación de duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviewsGoogle.drop_duplicates(subset=['user_id', 'time', 'gmap_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#si tiene texto\n",
    "df_reviewsGoogle['has_text'] = df_reviewsGoogle['text'].apply(lambda x: 1 if isinstance(x, str) and len(x.strip()) > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cantidad de fotos\n",
    "df_reviewsGoogle['num_pics'] = df_reviewsGoogle['pics'].apply(lambda x: len(x) if isinstance(x, list) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#si tiene respuesta\n",
    "df_reviewsGoogle['has_resp'] = df_reviewsGoogle['resp'].apply(lambda x: 1 if isinstance(x, dict) and len(x) > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtro por los negocios de la ciudad\n",
    "df_reviewsGoogle_ML = df_reviewsGoogle[df_reviewsGoogle['gmap_id'].isin(negocios)][['user_id', 'time', 'rating', 'gmap_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esportar a un csv\n",
    "df_reviewsGoogle_ML.to_csv('Archivos/reviewsGoogle_ML.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminar columnas innecesarias\n",
    "df_reviewsGoogle.drop(columns=['name', 'text', 'pics', 'resp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esportar a un parquet\n",
    "df_reviewsGoogle.to_parquet('Archivos/ReviewsGoogle.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminacion de dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_reviewsGoogle\n",
    "del df_reviewsGoogle_ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business - YELP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar los archivos desde la nube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La carpeta BusinessYelp ya existe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1byFtzpZXopdCN-XYmMHMpZqzgAqfQBBu\n",
      "From (redirected): https://drive.google.com/uc?id=1byFtzpZXopdCN-XYmMHMpZqzgAqfQBBu&confirm=t&uuid=ad75f79a-9907-4c9b-90b3-ad1ac84452d3\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\BusinessYelp\\business.pkl\n",
      "100%|██████████| 116M/116M [00:51<00:00, 2.27MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'BusinessYelp/business.pkl' descargado correctamente.\n",
      "Archivo 'BusinessYelp/business.pkl' cargado en el DataFrame correctamente.\n"
     ]
    }
   ],
   "source": [
    "# ID del archivo de Google Drive\n",
    "file_id = '1byFtzpZXopdCN-XYmMHMpZqzgAqfQBBu'\n",
    "\n",
    "# Construir la URL de descarga\n",
    "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "# Nombre de la carpeta que quieres crear\n",
    "carpeta_destino = 'BusinessYelp'\n",
    "\n",
    "# Verificar si la carpeta existe\n",
    "if not os.path.exists(carpeta_destino):\n",
    "    # Crear la carpeta si no existe\n",
    "    os.makedirs(carpeta_destino)\n",
    "    print(f\"Se ha creado la carpeta: {carpeta_destino}\")\n",
    "else:\n",
    "    print(f\"La carpeta {carpeta_destino} ya existe.\")\n",
    "\n",
    "# Nombre del archivo local\n",
    "file_path = \"BusinessYelp/business.pkl\"\n",
    "\n",
    "# Verificar si el archivo ya está descargado\n",
    "if not os.path.exists(file_path):\n",
    "    try:\n",
    "        # Descargar el archivo usando gdown\n",
    "        gdown.download(download_url, file_path, quiet=False)\n",
    "        print(f\"Archivo '{file_path}' descargado correctamente.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al descargar el archivo: {e}\")\n",
    "\n",
    "# Cargar el archivo pickle en un DataFrame de pandas si existe\n",
    "if os.path.exists(file_path):\n",
    "    try:\n",
    "        df_business = pd.read_pickle(file_path)\n",
    "        print(f\"Archivo '{file_path}' cargado en el DataFrame correctamente.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el archivo en el DataFrame: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"El archivo '{file_path}' no está disponible para cargar en el DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminacion de duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n",
       "       'latitude', 'longitude', 'stars', 'review_count', 'is_open',\n",
       "       'attributes', 'categories', 'hours', 'business_id', 'name', 'address',\n",
       "       'city', 'state', 'postal_code', 'latitude', 'longitude', 'stars',\n",
       "       'review_count', 'is_open', 'attributes', 'categories', 'hours'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_business.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business.drop_duplicates(subset=['business_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elimino columnas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renombro y saco columnas\n",
    "df_business.columns=['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n",
    "       'latitude', 'longitude', 'stars', 'review_count', 'is_open',\n",
    "       'attributes', 'categories', 'hours', 'business_id2', 'name2', 'address2',\n",
    "       'city2', 'state2', 'postal_code2', 'latitude2', 'longitude2', 'stars2',\n",
    "       'review_count2', 'is_open2', 'attributes2', 'categories2', 'hours2']\n",
    "\n",
    "df_business.drop(columns=['business_id2', 'name2', 'address2',\n",
    "       'city2', 'state2', 'postal_code2', 'latitude2', 'longitude2', 'stars2',\n",
    "       'review_count2', 'is_open2', 'attributes2', 'categories2', 'hours2'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado por california"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luego, aplica el filtro para las empresas en California (por código postal)\n",
    "df_business_CA = df_business[(df_business['postal_code'] >= '90000') & (df_business['postal_code'] <= '96612')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado por Los Angeles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business_LA = df_business_CA[(df_business_CA['postal_code'] >= '90000') & (df_business_CA['postal_code'] <= '91609')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de los Datos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina valores nulos en las columnas attributes y hours\n",
    "df_business_CA= df_business_CA.dropna(subset=['attributes', 'hours'])\n",
    "\n",
    "# Modificación del tipo  de datos en las columnas 'latitude','longitud','stars','review_count','is-open'\n",
    "\n",
    "df_business_CA['latitude'] = pd.to_numeric(df_business_CA['latitude'], errors='coerce')\n",
    "df_business_CA['longitude'] = pd.to_numeric(df_business_CA['longitude'], errors='coerce')\n",
    "df_business_CA['stars'] = pd.to_numeric(df_business_CA['stars'], errors='coerce')\n",
    "df_business_CA['review_count'] = pd.to_numeric(df_business_CA['review_count'], errors='coerce')\n",
    "df_business_CA['is_open'] = pd.to_numeric(df_business_CA['is_open'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminacion de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business_CA.drop(columns=['is_open'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimino columnas que no se usan del DF dpara ML\n",
    "df_business_LA.drop(columns=['state', 'postal_code','is_open','hours'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardo el archivo parquet, para poder importarlo si es necesario\n",
    "df_business_CA.to_parquet('Archivos/business_CA_Yelp.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv para ML\n",
    "df_business_LA.to_csv('Archivos/business_ML.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listado de negocios de YELP\n",
    "businessCA=df_business_CA['business_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "businessCA.to_csv('Archivos/yelpCA.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "businessML=df_business_LA['business_id']\n",
    "businessML.to_csv('Archivos/BusinesslistYELPML.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review - YELP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importacion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha creado la carpeta: ReviewYelp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1mwNNdOMSNty6WumYdH9FJNJZJYQ6oD1c\n",
      "From (redirected): https://drive.google.com/uc?id=1mwNNdOMSNty6WumYdH9FJNJZJYQ6oD1c&confirm=t&uuid=2ad51967-6120-42a6-928c-9b47c367650e\n",
      "To: c:\\Users\\londe\\OneDrive\\Programacion\\1 - Henrry\\Data Science\\CLASES - DATA\\PROYECTO FINAL\\PFH_Google_Yelp\\ReviewYelp\\ReviewYelp.json\n"
     ]
    }
   ],
   "source": [
    "# ID del archivo de Google Drive\n",
    "file_id = '1mwNNdOMSNty6WumYdH9FJNJZJYQ6oD1c'\n",
    "\n",
    "# URL de descarga directa desde Google Drive\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "# Nombre de la carpeta que quieres crear\n",
    "carpeta_destino = 'ReviewYelp'\n",
    "\n",
    "# Verificar si la carpeta existe\n",
    "if not os.path.exists(carpeta_destino):\n",
    "    # Crear la carpeta si no existe\n",
    "    os.makedirs(carpeta_destino)\n",
    "    print(f\"Se ha creado la carpeta: {carpeta_destino}\")\n",
    "else:\n",
    "    print(f\"La carpeta {carpeta_destino} ya existe.\")\n",
    "\n",
    "# Nombre del archivo descargado y ruta de salida\n",
    "output = 'ReviewYelp/ReviewYelp.json'\n",
    "file_path = output\n",
    "\n",
    "# Verificar si el archivo ya está descargado\n",
    "if not os.path.exists(file_path):\n",
    "    try:\n",
    "        # Descargar el archivo\n",
    "        gdown.download(url, output, quiet=False)\n",
    "        print(f\"Archivo '{output}' descargado correctamente.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al descargar el archivo: {e}\")\n",
    "\n",
    "# Procesar el archivo JSON en chunks y guardar en un DataFrame final\n",
    "try:\n",
    "    # Definir el tamaño del chunk (número de líneas por chunk)\n",
    "    chunk_size = 1000\n",
    "\n",
    "    # Lista para almacenar los DataFrames procesados\n",
    "    results = []\n",
    "\n",
    "    # Leer el archivo JSON en chunks\n",
    "    chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size)\n",
    "\n",
    "    # Procesar cada chunk por separado\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Ejemplo de procesamiento: mostrar las primeras filas del chunk\n",
    "        print(f\"Procesando chunk {i + 1}\")\n",
    "        print(chunk.head())\n",
    "\n",
    "        # Guardar resultados intermedios en una lista\n",
    "        results.append(chunk)\n",
    "\n",
    "        # Liberar memoria si no necesitas almacenar los chunks completos\n",
    "        del chunk\n",
    "\n",
    "    # Combinar todos los chunks procesados en un solo DataFrame final\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "    print(\"Procesamiento completo. DataFrame final creado.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al procesar el archivo JSON: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtros de dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businessCA=pd.read_csv('Archivos/yelpCA.csv')\n",
    "businessML=pd.read_csv('Archivos/businesslistYELPML.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewYELP=final_df[final_df['business_id'].isin(businessCA['business_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewYELPML=final_df[final_df['business_id'].isin(businessML['business_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewYELPML.to_csv('Archivos/ReviewsYELP_ML.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewYELP.to_parquet('Archivos/ReviewYELP.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USER - YELP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID del archivo de Google Drive\n",
    "file_id = '1TT4ARRIV6i2fO1b5yb0aSUkjhxMb9u6g'\n",
    "\n",
    "# URL de descarga directa desde Google Drive\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "# Nombre de la carpeta que quieres crear\n",
    "carpeta_destino = 'UserYelp'\n",
    "\n",
    "# Verificar si la carpeta existe\n",
    "if not os.path.exists(carpeta_destino):\n",
    "    # Crear la carpeta si no existe\n",
    "    os.makedirs(carpeta_destino)\n",
    "    print(f\"Se ha creado la carpeta: {carpeta_destino}\")\n",
    "else:\n",
    "    print(f\"La carpeta {carpeta_destino} ya existe.\")\n",
    "\n",
    "# Nombre del archivo descargado\n",
    "output = 'UserYelp/UserYelp.parquet'\n",
    "\n",
    "# Verificar si el archivo ya está descargado\n",
    "if not os.path.exists(output):\n",
    "    try:\n",
    "        # Descargar el archivo si no está presente localmente\n",
    "        gdown.download(url, output, quiet=False)\n",
    "        print(f\"Archivo '{output}' descargado correctamente.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al descargar el archivo: {e}\")\n",
    "\n",
    "# Leer el archivo Parquet en un DataFrame si existe\n",
    "if os.path.exists(output):\n",
    "    try:\n",
    "        df = pd.read_parquet(output)\n",
    "        print(f\"Archivo '{output}' cargado en el DataFrame correctamente.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el archivo en el DataFrame: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"El archivo '{output}' no está disponible para cargar en el DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtro ciudad elegida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se importa el archivo usado\n",
    "df_rev_YELP_ML=pd.read_csv('Archivos/ReviewsYELP_ML.csv')\n",
    "#se toman los valores de userid\n",
    "listado=df_rev_YELP_ML['user_id'].unique()\n",
    "#eliminar df\n",
    "del df_rev_YELP_ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular valores nulos y porcentajes\n",
    "valores_nulos = df.isnull().sum()\n",
    "# Contar las ocurrencias de cada fila duplicada\n",
    "duplicate_counts = df[df.duplicated()].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribir el DataFrame como un archivo Parquet\n",
    "filtered_reviews_ca.to_parquet('Archivos/user_ca.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga (LOAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archivos para Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_ML=df[df['user_id'].isin(listado)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_ML.drop(columns=['compliment_hot',\n",
    "       'compliment_more', 'compliment_profile', 'compliment_cute',\n",
    "       'compliment_list', 'compliment_note', 'compliment_plain',\n",
    "       'compliment_cool', 'compliment_funny', 'compliment_writer',\n",
    "       'compliment_photos'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_ML.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_ML.to_csv('Archivos/user_ML.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear ETL local como script de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script ETL.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCP Bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run cloud_up.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
